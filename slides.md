# What is Drift?
## â€œWhen the world changes, but the model doesnâ€™t.â€
Drift is a change in the joint data distribution P(X,Y) over time.

--

# Types of Drift
## By what changes

<img src="figures/real_virtual_drift.png" alt="Drift types" style="width:70%">

Note:

Î‘Ï‚ Ï†Î±Î½Ï„Î±ÏƒÏ„Î¿ÏÎ¼Îµ Î­Î½Î± Î¼Î¿Î½Ï„Î­Î»Î¿ Ï€Î¿Ï… Î±Î½Î¹Ï‡Î½ÎµÏÎµÎ¹ Î±Ï€Î¬Ï„Î· ÏƒÎµ ÏƒÏ…Î½Î±Î»Î»Î±Î³Î­Ï‚.
Î‘Î½ Î±Î»Î»Î¬Î¾ÎµÎ¹ Î¼ÏŒÎ½Î¿ Î¿ Ï„ÏÏŒÏ€Î¿Ï‚ Ï€Î¿Ï… Î¼Î¿Î¹Î¬Î¶Î¿Ï…Î½ Î¿Î¹ ÏƒÏ…Î½Î±Î»Î»Î±Î³Î­Ï‚ â€” Î³Î¹Î± Ï€Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î±, Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎµÏ‚ online Î±Î³Î¿ÏÎ­Ï‚ Ï„Î± Î§ÏÎ¹ÏƒÏ„Î¿ÏÎ³ÎµÎ½Î½Î± â€” Î­Ï‡Î¿Ï…Î¼Îµ virtual drift: Î±Î»Î»Î¬Î¶ÎµÎ¹ Î· ÎºÎ±Ï„Î±Î½Î¿Î¼Î® Ï„Ï‰Î½ ÎµÎ¹ÏƒÏŒÎ´Ï‰Î½, ÏŒÏ‡Î¹ Î· ÏƒÏ‡Î­ÏƒÎ· Î¼Îµ Ï„Î·Î½ Î±Ï€Î¬Ï„Î·.
Î‘Î½ ÏŒÎ¼Ï‰Ï‚ Î¿Î¹ Î±Ï€Î±Ï„ÎµÏÎ½ÎµÏ‚ Î±Î»Î»Î¬Î¾Î¿Ï…Î½ ÏƒÏ„ÏÎ±Ï„Î·Î³Î¹ÎºÎ® ÎºÎ±Î¹ ÎºÎ¬Î½Î¿Ï…Î½ Î¼Î¹ÎºÏÏŒÏ„ÎµÏÎµÏ‚, Ï€Î¹Î¿ â€œÎºÎ±Î½Î¿Î½Î¹ÎºÎ­Ï‚â€ ÏƒÏ…Î½Î±Î»Î»Î±Î³Î­Ï‚, Î±Î»Î»Î¬Î¶ÎµÎ¹ Î· ÏƒÏ‡Î­ÏƒÎ· Î¼ÎµÏ„Î±Î¾Ï ÎµÎ¹ÏƒÏŒÎ´Î¿Ï… ÎºÎ±Î¹ ÎµÎ¾ÏŒÎ´Î¿Ï… â€” Î±Ï…Ï„ÏŒ ÎµÎ¯Î½Î±Î¹ actual drift.
ÎšÎ±Î¹ Î±Î½ Î±Î»Î»Î¬Î¾Î¿Ï…Î½ ÎºÎ±Î¹ Î¿Î¹ Î´ÏÎ¿ Ï€Î»ÎµÏ…ÏÎ­Ï‚, Î´Î·Î»Î±Î´Î® ÎºÎ±Î¹ Î· ÏƒÏ…Î¼Ï€ÎµÏÎ¹Ï†Î¿ÏÎ¬ Ï„Ï‰Î½ Ï‡ÏÎ·ÏƒÏ„ÏÎ½ ÎºÎ±Î¹ Ï„Ï‰Î½ Î±Ï€Î±Ï„ÎµÏÎ½Ï‰Î½, Ï„ÏŒÏ„Îµ Î­Ï‡Î¿Ï…Î¼Îµ concept drift.

--

# Types of Drift
## By how it changes

<img src="figures/drift_classes.png" alt="Drift types_in_time" style="width:70%">

--

# Why do we Detect Drift?
- Detect changes in data or concept $\rightarrow$ model adaptation (e.g. retraining)
- Ensure the model remains accurate and relevant as the world changes

---

# How do we Detect Drift?

Process called drift detection which monitors a system over time to spot changes in its behavior or performance.

Note: It helps identify when things start to deviate from expected standardsâ€”like a model becoming less accurateâ€”so that adjustments can be made to keep quality high.

--

# Supervised Drift Detection

Algorithms assume that true labels are available in order for the model to be trained

--

# Semi-supervised Drift Detection

These algorithms useÂ **both features and labels**Â when labels areÂ **partially available**

Note:

Labels are often delayed, sparse, or costly to obtain

--

# Unsupervised Drift Detection

Unsupervised drift detection algorithms relyÂ **only on the input features**Â $X$

---

# Drift Detection Categories

--

# Block-based Methods
<!-- ## â€œAnalyze the story in chaptersâ€ -->

Divides the data stream into blocks.
Each block is analyzed individually.

## How it works:

- In each block runs a change point algorithm

## Pros:

- Robust to noise
- Can detect multiple drifts within a block

## Cons:

- Slow reaction time (waits for block completion)
- Computationally heavy

Note:

# Change point algorithm

Looks for positions where the statistical distribution before and after changes significantly

Given a time-ordered sequence x_1, x_2,â€¦,x_T, the algorithm tries to identify time 
indices Ï„_1 ,Ï„_2, â€¦, Ï„_k where something fundamental in the data changes â€” like its 
mean, variance, correlation, or probability distribution

# Heavy

- Process large blocks of data at once,

- Often analyze the entire block holistically, not just sample summaries,

- And may search for multiple change points within the block

Examples: Kernel MMD, Wasserstein-based detectors

## Key distinction:

Block-based methods use large, mostly non-overlapping windows analyzed as independent data segments, not as continuously moving windows

--

# Batch-based Methods
<!-- ## â€œCompare yesterday and todayâ€ -->

They keep two smaller windows of data:
- A reference window (past concept)
- A detection window (recent data)

## How it works:

- Windows can be fixed, sliding, or adaptive
- When differences exceed a threshold $\rightarrow$ drift detected

## Pros:

- Detects both gradual and sudden drifts

## Cons:

- Choice of window size impacts sensitivity

Note:

They compare distributions (e.g., KS test, MMD) to check if the new batch differs significantly from the old one

Examples: NN-DVI, SQSI.

## Key distinction:

Batch-based methods rely on explicit comparison between two consecutive windows, focusing on contrast between past and present data.

--

# Online-based Methods
<!-- ## â€œReact as the words changeâ€ -->

They detect drift continuously, updating statistics as each instance arrives

## How it works:

- Use sliding or adaptive window that is updated with every new data point

- Apply test statistics or hypothesis tests in real time

## Pros:

- Fastest detection, reacts immediately to change

## Cons:

- Sensitive to noise, prone to false alarms

Note:

1. Receive a new data point x_t

2. Update the current statistic (mean, variance, error, etc.) using x_t.

3. Compute a test statistic to check for significant change.

4. Trigger drift if the test statistic exceeds a threshold.

5. Reset or shrink the window if drift is detected.

# Test Statistic

A test statistic is simply a numerical measure of how much the current data deviates from whatâ€™s expected under the â€œno driftâ€ assumption.

Then the detector checks: T_t > thresholdâ‡’driftÂ detected

- Mean-based statistics (Page-Hinkley)
- Variance-based statistics
- Distance-based statistics


Batch-based:

- Wait until you have 100 samples in the detection window.
- Compare it to the previous 100 samples (reference).
- Perform the test â†’ decide drift or no drift.
- Slide the window (maybe by 10 or 50 samples) â†’ repeat.

ğŸ‘‰ Detection only happens after every batch, e.g., every 100 samples.

ğŸ”¹ Online-based:

- As soon as each new sample arrives:
- Update your statistic (mean, error, likelihood, etc.).
- Check if it deviates significantly.
- If so â†’ drift.

ğŸ‘‰ Detection happens continuously, potentially every single data point.

# ADWIN

ADWIN (ADaptive WINdowing) maintains a dynamically sized window W of recent data.

1. When a new instance arrives, it adds it to W.
2. It splits W into two sub-windows W_1 and W_2.
3. It compares their means using a Hoeffding bound to test if they differ significantly.
4. If yes â†’ drift detected, and old data (the earlier part of the window) is dropped.

Examples: ADWIN, DSDD, Page-Hinkley, SAND.

## Key distinction:

Online-based methods differ mainly in timing â€” they update continuously per instance, unlike batch or block methods that wait for windows to fill.

---

# Dynamic Adapting Window Independence Drift Detection (DAWIDD)

Turns concept drift into an independence test

Note:

The whole philosophy behind DAWIDD is very simple but powerful:
if the data distribution does not change over time, then the data ğ‘‹ and time ğ‘‡ should be independent.
But if the data distribution does change â€” meaning a concept drift has occurred â€” then ğ‘‹ and ğ‘‡ become dependent.

--

# 1. Attach Time to Each Sample

Transform our data stream into a sequence of pairs, each data point is now represented as ($x_i$, $t_i$), where $x_i$ is the feature vector and $t_i$ is the timestamp

--

# 2. Maintain a Dynamically Adapting Window

This window, $W$, is constantly updated:

- Each time a new data point arrives, itâ€™s added to the window

- The window size is controlled by two parameters:

  - $n_{min}$: the minimum number of samples needed before we can reliably run a statistical test

  - $n_{max}$: the maximum number of samples allowed before we start dropping old ones.

Once the window exceeds $n_{max}$, we randomly delete some points rather than just discarding the oldest.

--

# 3. Testing Independence Between X and T

Once we have enough samples in our window (at least $n_{min}$), we perform a statistical test to check whether the data values $X$ are independent of the time values $T$.

- If the $H_0$ holds, meaning $X$ and $T$ are independent, thereâ€™s **no** drift
  - Keep adding **new** samples to the window

- If $H_0$ is reject, that means the distribution of $X$ has changed with time. So, a drift is detected
  - We signal a **drift event**
  - **Shrink** the window down to a small size, typically keeping only the last $n_{min}$ samples


Note: 

- chi-square test for independence

- Hilbertâ€“Schmidt Independence Criterion